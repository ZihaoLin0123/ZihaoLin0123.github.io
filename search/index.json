[{"content":"Introduction This is a summary of many kinds of activation functions.\nActivation Functions I. Sigmoid Function \\[\\sigma (x) = \\frac{1}{1+e^{-x}}\\]\n\\[\\frac{\\partial \\sigma (x)}{\\partial x} = \\sigma (x)(1-\\sigma (x))\\]\n Output range: [0, 1] metrics: Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron. Demetrics:  Saturated neurons \u0026quot;kill\u0026quot; the gradients. When x becomes extremely large or negative large, the gradient will become nearly zero. When x equals to zero, it will be fine. Sigmoid outputs are not zero-centered. When the inputs are always positive, the gradient is the multiply of the local gradients of parameters W, which equals to X, and the upstream gradient coming down; therefore, the sign of the gradient will always be the sign of upstream gradient. This means the gradients on W are always going to move in the same direction. \\[\\frac{\\partial L}{\\partial \\omega} = \\sigma (\\sum_i \\omega_ix_i + b)(1-\\sigma (\\sum_i \\omega_ix_i + b))x \\cdot \\text(upstream_gradient)\\] $exp()$ is a bit compute expensive.   II. Tanh(x)  Output range: Metrics: zero centered Demetrics: still kills gradients when saturated.  III. ReLU \\[f(x) = max(0, x)\\]\n Output range: [0, $+\\infty$] Metrics:  Does not saturate (in +region). It is very computationally efficient. It converges much faster than sigmoid/tanh in practive. (e.g. 6x)  Demetrics:  Not zero-centered outputs An annoyance: in the negative region, the gradients will become zero. Sometimes the ReLU will be dead. Therefore, people like to initialize ReLU neurons with slightly positive biases (e.g. 0.01).    Reference [1] http://cs231n.stanford.edu/slides/2020/lecture_7.pdf [2]\n","date":"2021-02-21T17:14:13-05:00","image":"https://ZihaoLin0123.github.io/helena-hertz-wWZzXlDpMog-unsplash.jpg","permalink":"https://ZihaoLin0123.github.io/p/deep-learning-1-activation-function/","title":"Deep Learning (1) -- Activation Function"},{"content":"This is a test!\n","date":"2020-02-20T14:40:55-05:00","permalink":"https://ZihaoLin0123.github.io/p/test1/","title":"Test1"}]
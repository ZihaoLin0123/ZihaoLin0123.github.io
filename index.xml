<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Zihao Lin&#39;s Blog</title>
        <link>https://ZihaoLin0123.github.io/</link>
        <description>Recent content on Zihao Lin&#39;s Blog</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 21 Feb 2021 17:14:13 -0500</lastBuildDate><atom:link href="https://ZihaoLin0123.github.io/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>Deep Learning (1) -- Activation Function</title>
        <link>https://ZihaoLin0123.github.io/p/deep-learning-1-activation-function/</link>
        <pubDate>Sun, 21 Feb 2021 17:14:13 -0500</pubDate>
        
        <guid>https://ZihaoLin0123.github.io/p/deep-learning-1-activation-function/</guid>
        <description>&lt;img src="https://ZihaoLin0123.github.io/helena-hertz-wWZzXlDpMog-unsplash.jpg" alt="Featured image of post Deep Learning (1) -- Activation Function" /&gt;&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;This is a summary of many kinds of activation functions.&lt;/p&gt;

&lt;h2 id=&#34;activation-functions&#34;&gt;Activation Functions&lt;/h2&gt;

&lt;h3 id=&#34;i-sigmoid-function&#34;&gt;I. Sigmoid Function&lt;/h3&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\sigma (x) = \frac{1}{1+e^{-x}}\]&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[\frac{\partial \sigma (x)}{\partial x} = \sigma (x)(1-\sigma (x))\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Output range: [0, 1]&lt;/li&gt;
&lt;li&gt;metrics: Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron.&lt;/li&gt;
&lt;li&gt;Demetrics:

&lt;ol&gt;
&lt;li&gt;Saturated neurons &amp;quot;kill&amp;quot; the gradients. When x becomes extremely large or negative large, the gradient will become nearly zero. When x equals to zero, it will be fine.&lt;/li&gt;
&lt;li&gt;Sigmoid outputs are not zero-centered. When the inputs are always positive, the gradient is the multiply of the local gradients of parameters W, which equals to X, and the upstream gradient coming down; therefore, the sign of the gradient will always be the sign of upstream gradient. This means the gradients on W are always going to move in the same direction.
&lt;span  class=&#34;math&#34;&gt;\[\frac{\partial L}{\partial \omega} = \sigma (\sum_i \omega_ix_i + b)(1-\sigma (\sum_i \omega_ix_i + b))x \cdot \text(upstream_gradient)\]&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;$exp()$ is a bit compute expensive.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;ii-tanhx&#34;&gt;II. Tanh(x)&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;Output range:&lt;/li&gt;
&lt;li&gt;Metrics: zero centered&lt;/li&gt;
&lt;li&gt;Demetrics: still kills gradients when saturated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;iii-relu&#34;&gt;III. ReLU&lt;/h3&gt;

&lt;p&gt;&lt;span  class=&#34;math&#34;&gt;\[f(x) = max(0, x)\]&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Output range: [0, $+\infty$]&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;reference&#34;&gt;Reference&lt;/h3&gt;

&lt;p&gt;[1] &lt;a href=&#34;http://cs231n.stanford.edu/slides/2020/lecture_7.pdf&#34;&gt;http://cs231n.stanford.edu/slides/2020/lecture_7.pdf&lt;/a&gt;
[2]&lt;/p&gt;
</description>
        </item>
        <item>
        <title>Test1</title>
        <link>https://ZihaoLin0123.github.io/p/test1/</link>
        <pubDate>Thu, 20 Feb 2020 14:40:55 -0500</pubDate>
        
        <guid>https://ZihaoLin0123.github.io/p/test1/</guid>
        <description>&lt;p&gt;This is a test!&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
